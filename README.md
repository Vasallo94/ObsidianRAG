# ObsidianRAG ğŸ§ 

Sistema RAG (Retrieval-Augmented Generation) para consultar tus notas de Obsidian usando **LangGraph** y **LLMs locales** con Ollama. Pregunta en lenguaje natural y obtÃ©n respuestas basadas en tu conocimiento personal.

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![LangGraph](https://img.shields.io/badge/LangGraph-0.2+-green)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLMs-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

<p align="center">
  <img src="img/demo.gif" alt="Demo" width="600">
</p>

---

## ğŸ“‹ Tabla de Contenidos

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Requisitos Previos](#-requisitos-previos)
- [InstalaciÃ³n RÃ¡pida](#-instalaciÃ³n-rÃ¡pida)
- [ConfiguraciÃ³n](#-configuraciÃ³n)
- [Uso](#-uso)
- [Arquitectura](#-arquitectura)
- [Modelos Disponibles](#-modelos-disponibles)
- [SoluciÃ³n de Problemas](#-soluciÃ³n-de-problemas)
- [Contribuir](#-contribuir)

---

## âœ¨ CaracterÃ­sticas

### ğŸ” BÃºsqueda HÃ­brida Avanzada
- **Vectorial + BM25**: Combina embeddings semÃ¡nticos con bÃºsqueda lÃ©xica
- **CrossEncoder Reranker**: BAAI/bge-reranker-v2-m3 para reordenar por relevancia
- **GraphRAG**: ExpansiÃ³n de contexto siguiendo enlaces `[[wikilinks]]` de Obsidian

### ğŸ¤– IntegraciÃ³n LLM
- **100% Local**: Todo corre en tu mÃ¡quina, sin enviar datos a la nube
- **MÃºltiples modelos**: Selector en UI para cambiar entre gemma3, qwen2.5, qwen3, deepseek-r1
- **Fallback inteligente**: Si un modelo no estÃ¡ disponible, usa alternativas automÃ¡ticamente

### ğŸ“Š AnÃ¡lisis y MÃ©tricas
- **Scores de relevancia**: Cada fuente muestra su score de reranker (0-100%)
- **Logging detallado**: Trazabilidad completa de cada consulta
- **IndexaciÃ³n incremental**: Solo procesa notas modificadas

---

## ğŸ“¦ Requisitos Previos

### 1. Python 3.11+

```bash
# Verificar versiÃ³n
python --version  # Debe ser 3.11 o superior
```

### 2. Ollama

Ollama es el motor de LLMs locales. InstÃ¡lalo desde [ollama.ai](https://ollama.ai/):

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Descarga desde https://ollama.com/download
```

Verifica que funcione:
```bash
ollama --version
```

### 3. UV (Gestor de paquetes recomendado)

```bash
# macOS/Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# O con pip
pip install uv
```

---

## ğŸš€ InstalaciÃ³n RÃ¡pida

### Paso 1: Clonar el repositorio

```bash
git clone https://github.com/Vasallo94/ObsidianRAG.git
cd ObsidianRAG
```

### Paso 2: Instalar dependencias

```bash
uv sync
```

### Paso 3: Configurar variables de entorno

```bash
# Copiar plantilla
cp .env.example .env

# Editar con tu editor favorito
nano .env  # o code .env, vim .env, etc.
```

**Contenido mÃ­nimo de `.env`:**
```env
# OBLIGATORIO: Ruta a tu vault de Obsidian
OBSIDIAN_PATH=/Users/tu_usuario/Documents/ObsidianVault

# OPCIONAL: Modelo LLM (default: gemma3)
LLM_MODEL=gemma3
```

### Paso 4: Descargar modelos de Ollama

```bash
# Iniciar Ollama (si no estÃ¡ corriendo)
ollama serve &

# Descargar modelo LLM (elige uno)
ollama pull gemma3      # Recomendado, equilibrado
ollama pull qwen2.5     # Bueno para espaÃ±ol
ollama pull qwen3       # Mejor razonamiento
ollama pull deepseek-r1 # Razonamiento avanzado

# OPCIONAL: Modelo de embeddings de Ollama
ollama pull embeddinggemma  # 622MB, multilingÃ¼e
```

> **Nota**: Si no descargas `embeddinggemma`, el sistema usarÃ¡ automÃ¡ticamente HuggingFace embeddings (se descargan automÃ¡ticamente la primera vez).

### Paso 5: Iniciar el servidor

```bash
uv run cerebro.py
```

DeberÃ­as ver:
```
INFO - âœ… AplicaciÃ³n iniciada exitosamente
INFO - Uvicorn running on http://0.0.0.0:8000
```

### Paso 6: Abrir la interfaz web

```bash
# En otra terminal
uv run streamlit run app.py
```

Abre tu navegador en: **http://localhost:8501**

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno Completas

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
# ============ OBLIGATORIO ============
OBSIDIAN_PATH=/ruta/a/tu/vault

# ============ MODELOS ============
# LLM (gemma3, qwen2.5, qwen3, deepseek-r1)
LLM_MODEL=gemma3

# Embeddings: 'ollama' o 'huggingface'
EMBEDDING_PROVIDER=ollama
OLLAMA_EMBEDDING_MODEL=embeddinggemma

# Si usas HuggingFace (fallback automÃ¡tico)
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2

# ============ RERANKER ============
USE_RERANKER=true
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_TOP_N=6

# ============ RETRIEVAL ============
CHUNK_SIZE=1500
CHUNK_OVERLAP=300
RETRIEVAL_K=12
BM25_K=5
BM25_WEIGHT=0.4
VECTOR_WEIGHT=0.6

# ============ API ============
API_HOST=0.0.0.0
API_PORT=8000
```

### Archivo .env.example

El proyecto incluye un `.env.example` con todos los valores por defecto.

---

## ğŸ“– Uso

### Interfaz Web (Recomendado)

1. Inicia el servidor: `uv run cerebro.py`
2. Inicia la UI: `uv run streamlit run app.py`
3. Abre http://localhost:8501
4. Â¡Pregunta sobre tus notas!

**CaracterÃ­sticas de la UI:**
- ğŸ¤– Selector de modelo LLM en el sidebar
- ğŸ“š Fuentes con scores de relevancia
- ğŸ”„ BotÃ³n de reindexar base de datos
- ğŸ—‘ï¸ BotÃ³n de limpiar chat

### API REST

```bash
# Hacer una pregunta
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"text": "Â¿QuÃ© notas tengo sobre Python?", "model": "gemma3"}'

# Verificar estado
curl http://localhost:8000/health

# Obtener estadÃ­sticas
curl http://localhost:8000/stats

# Forzar reindexaciÃ³n
curl -X POST http://localhost:8000/rebuild_db
```

### Respuesta de la API

```json
{
  "question": "Â¿QuÃ© notas tengo sobre Python?",
  "result": "SegÃºn tus notas, tienes documentaciÃ³n sobre...",
  "sources": [
    {
      "source": "ProgramaciÃ³n/Python Basics.md",
      "score": 0.92,
      "retrieval_type": "retrieved"
    },
    {
      "source": "ProgramaciÃ³n/Django Tutorial.md", 
      "score": 0.78,
      "retrieval_type": "graphrag_link"
    }
  ],
  "process_time": 2.5,
  "session_id": "abc123..."
}
```

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚  Streamlit  â”‚  â—„â”€â”€ Interfaz web interactiva              â”‚
â”‚  â”‚    (UI)     â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTP
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BACKEND                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚         LangGraph Agent         â”‚   â”‚
â”‚  â”‚  (cerebro)   â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚Retrieve â”‚â”€â”€â”€â–¶â”‚ Generate  â”‚   â”‚   â”‚
â”‚                       â”‚  â”‚  Node   â”‚    â”‚   Node    â”‚   â”‚   â”‚
â”‚                       â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚               â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      RETRIEVAL       â”‚                        â”‚         LLM          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚  EnsembleRetrieverâ”‚ â”‚                        â”‚  â”‚     Ollama     â”‚  â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚ â”‚                        â”‚  â”‚  (gemma3, etc) â”‚  â”‚
â”‚ â”‚ â”‚Vectorâ”‚ â”‚BM25 â”‚ â”‚ â”‚                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â”‚ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â”‚ â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜    â”‚ â”‚
â”‚ â”‚         â–¼        â”‚ â”‚
â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚ â”‚  â”‚  Reranker  â”‚  â”‚ â”‚
â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚    ChromaDB      â”‚ â”‚
â”‚ â”‚   (Vectores)     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos

1. **Usuario** hace pregunta â†’ **Streamlit**
2. **Streamlit** â†’ POST `/ask` â†’ **FastAPI**
3. **FastAPI** â†’ invoca â†’ **LangGraph Agent**
4. **Retrieve Node**:
   - BÃºsqueda hÃ­brida (Vector + BM25)
   - Reranking con CrossEncoder
   - ExpansiÃ³n GraphRAG (sigue [[links]])
5. **Generate Node**:
   - Construye prompt con contexto
   - Invoca LLM (Ollama)
6. **Respuesta** â†’ FastAPI â†’ Streamlit â†’ Usuario

---

## ğŸ¤– Modelos Disponibles

### LLMs (Ollama)

| Modelo | TamaÃ±o | DescripciÃ³n | Comando |
|--------|--------|-------------|---------|
| `gemma3` | 5GB | Equilibrado, bueno para todo | `ollama pull gemma3` |
| `qwen2.5` | 4.4GB | Excelente para espaÃ±ol | `ollama pull qwen2.5` |
| `qwen3` | 5GB | Mejor razonamiento | `ollama pull qwen3` |
| `deepseek-r1` | 4.7GB | Razonamiento avanzado | `ollama pull deepseek-r1` |

### Embeddings

| Modelo | Provider | TamaÃ±o | DescripciÃ³n |
|--------|----------|--------|-------------|
| `embeddinggemma` | Ollama | 622MB | 100+ idiomas, rÃ¡pido |
| `paraphrase-multilingual-mpnet` | HuggingFace | 420MB | Fallback automÃ¡tico |

> **Tip**: El sistema hace fallback automÃ¡tico a HuggingFace si el modelo de Ollama no estÃ¡ disponible.

---

## ğŸ”§ SoluciÃ³n de Problemas

### âŒ "Ollama not available" / Connection refused

```bash
# 1. Verificar que Ollama estÃ¡ corriendo
ollama serve

# 2. Si usas macOS, puede estar como app
# Abre Ollama.app desde Aplicaciones

# 3. Verificar con
curl http://localhost:11434/api/tags
```

### âŒ "Model not found"

```bash
# Descargar el modelo que necesitas
ollama pull gemma3
ollama pull embeddinggemma  # Para embeddings
```

### âŒ "Collection does not exist" / DB corrupta

```bash
# Eliminar y reconstruir la base de datos
rm -rf db/
uv run cerebro.py
```

### âŒ Primera ejecuciÃ³n muy lenta

Es normal. La primera vez:
1. Descarga modelos de HuggingFace (reranker, embeddings)
2. Indexa todas tus notas de Obsidian
3. Crea la base de datos vectorial

Las siguientes ejecuciones son mucho mÃ¡s rÃ¡pidas (indexaciÃ³n incremental).

### âŒ "No se encontraron resultados"

1. Verifica que `OBSIDIAN_PATH` apunta a tu vault
2. AsegÃºrate de tener archivos `.md` en el vault
3. Reindexar: `rm -rf db/ && uv run cerebro.py`

### âŒ Respuestas en inglÃ©s cuando pregunto en espaÃ±ol

Prueba con `qwen2.5` que tiene mejor soporte para espaÃ±ol:
```bash
ollama pull qwen2.5
# Luego selecciÃ³nalo en la UI
```

---

## ğŸ“‚ Estructura del Proyecto

```
ObsidianRAG/
â”œâ”€â”€ cerebro.py              # ğŸ§  Servidor FastAPI (punto de entrada)
â”œâ”€â”€ app.py                  # ğŸ–¥ï¸ Interfaz Streamlit
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py         # âš™ï¸ ConfiguraciÃ³n Pydantic
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ qa_agent.py         # ğŸ¤– Agente LangGraph (retrieveâ†’generate)
â”‚   â”œâ”€â”€ qa_service.py       # ğŸ” Retriever hÃ­brido + reranker
â”‚   â”œâ”€â”€ db_service.py       # ğŸ’¾ ChromaDB + indexaciÃ³n
â”‚   â””â”€â”€ metadata_tracker.py # ğŸ“Š DetecciÃ³n de cambios
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py           # ğŸ“ ConfiguraciÃ³n de logging
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ debug/              # ğŸ› Utilidades de debug
â”‚   â””â”€â”€ tests/              # ğŸ§ª Tests de integraciÃ³n
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ styles.css          # ğŸ¨ Estilos de la UI
â”œâ”€â”€ db/                     # ğŸ’½ Base de datos ChromaDB (auto-generada)
â”œâ”€â”€ logs/                   # ğŸ“‹ Logs de ejecuciÃ³n
â”œâ”€â”€ .env                    # ğŸ” Variables de entorno (crear desde .env.example)
â””â”€â”€ .env.example            # ğŸ“„ Plantilla de configuraciÃ³n
```

---

## ğŸ”® Roadmap

- [x] Selector de modelos en UI
- [x] Fallback automÃ¡tico de embeddings
- [x] Scores de relevancia en fuentes
- [ ] Modo conversacional con memoria persistente
- [ ] Dashboard de analytics
- [ ] Soporte para APIs externas (Google AI, OpenAI)
- [ ] Exportar conversaciones

---

## ğŸ¤ Contribuir

Â¡Las contribuciones son bienvenidas!

1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-caracteristica`
3. Commit: `git commit -m 'feat: aÃ±adir nueva caracterÃ­stica'`
4. Push: `git push origin feature/nueva-caracteristica`
5. Abre un Pull Request

---

## ğŸ“„ Licencia

MIT License - ver [LICENSE](LICENSE)

---

## ğŸ™ CrÃ©ditos

- [LangGraph](https://github.com/langchain-ai/langgraph) - Framework de agentes
- [Ollama](https://ollama.ai/) - LLMs locales
- [ChromaDB](https://www.trychroma.com/) - Base de datos vectorial
- [Streamlit](https://streamlit.io/) - Framework de UI
- [Obsidian](https://obsidian.md/) - Tu segundo cerebro

---

<p align="center">
  Hecho con â¤ï¸ para la comunidad de Obsidian
</p>
