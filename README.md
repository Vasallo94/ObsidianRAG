# ObsidianRAG ğŸ§ 

Sistema RAG (Retrieval-Augmented Generation) para consultar notas de Obsidian usando **LangGraph** y **LLMs locales** con Ollama.

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![LangGraph](https://img.shields.io/badge/LangGraph-0.2+-green)
![Ollama](https://img.shields.io/badge/Ollama-Local_LLMs-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

## âœ¨ CaracterÃ­sticas

### ğŸ” BÃºsqueda HÃ­brida Avanzada
- **Vectorial + BM25**: Combina embeddings semÃ¡nticos con bÃºsqueda lÃ©xica
- **CrossEncoder Reranker**: BAAI/bge-reranker-v2-m3 para reordenar por relevancia
- **GraphRAG**: ExpansiÃ³n de contexto siguiendo enlaces `[[wikilinks]]` de Obsidian

### ğŸ¤– IntegraciÃ³n LLM
- **Ollama Local**: Modelos seleccionables (qwen2.5, qwen3, gemma3, deepseek-r1)
- **Sin dependencias cloud**: Todo corre localmente
- **Streaming deshabilitado**: Respuestas completas para mayor estabilidad

### ğŸ“Š AnÃ¡lisis y MÃ©tricas
- **Scores de relevancia**: Cada fuente muestra su score de reranker (0-100%)
- **Logging detallado**: Trazabilidad completa de cada consulta
- **IndexaciÃ³n incremental**: Solo procesa notas modificadas

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI    â”‚â”€â”€â”€â”€â–¶â”‚  LangGraph  â”‚
â”‚    (UI)     â”‚â—€â”€â”€â”€â”€â”‚   (API)      â”‚â—€â”€â”€â”€â”€â”‚   (Agent)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                    â”‚
                           â–¼                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ChromaDB   â”‚     â”‚   Ollama    â”‚
                    â”‚  (Vectores)  â”‚     â”‚   (LLM)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principales

| Archivo | DescripciÃ³n |
|---------|-------------|
| `cerebro.py` | Servidor FastAPI, punto de entrada principal |
| `app.py` | Interfaz Streamlit (opcional) |
| `services/qa_agent.py` | Agente LangGraph con nodos retrieveâ†’generate |
| `services/qa_service.py` | ConfiguraciÃ³n del retriever hÃ­brido |
| `services/db_service.py` | GestiÃ³n de ChromaDB e indexaciÃ³n |
| `config/settings.py` | ConfiguraciÃ³n centralizada (Pydantic) |

## ğŸš€ InstalaciÃ³n

### Requisitos Previos
- Python 3.11+
- [Ollama](https://ollama.ai/) instalado y corriendo
- UV (gestor de paquetes recomendado)

### Pasos

```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/ObsidianRAG.git
cd ObsidianRAG

# Instalar dependencias
uv sync

# Configurar variables de entorno
cp .env.example .env
# Editar .env con tu configuraciÃ³n
```

### Variables de Entorno (.env)

```env
# Ruta a tu vault de Obsidian
OBSIDIAN_PATH=/ruta/a/tu/vault

# Modelo LLM (Ollama)
LLM_MODEL=qwen2.5

# Modelo de embeddings
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2

# Reranker
USE_RERANKER=true
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_TOP_N=6

# Chunking
CHUNK_SIZE=1500
CHUNK_OVERLAP=300

# Retrieval
RETRIEVAL_K=12
BM25_K=5
BM25_WEIGHT=0.4
VECTOR_WEIGHT=0.6
```

## ğŸ“– Uso

### Iniciar el Servidor API

```bash
uv run cerebro.py
```

El servidor estarÃ¡ disponible en `http://localhost:8000`

### Iniciar la Interfaz Web (Opcional)

```bash
uv run streamlit run app.py
```

La UI estarÃ¡ en `http://localhost:8501`

### API Endpoints

#### POST /ask
Realiza una consulta al sistema RAG.

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿CuÃ¡les son mis notas sobre Python?"}'
```

**Respuesta:**
```json
{
  "answer": "SegÃºn tus notas...",
  "sources": [
    {
      "source": "ProgramaciÃ³n/Python Basics.md",
      "score": 0.92,
      "preview": "..."
    }
  ],
  "timing": {
    "total": 2.5,
    "retrieval": 0.8,
    "generation": 1.7
  }
}
```

#### GET /health
Verifica el estado del servidor.

#### GET /stats
Obtiene estadÃ­sticas de la base de datos.

## âš™ï¸ ConfiguraciÃ³n Avanzada

### ParÃ¡metros Clave (settings.py)

| ParÃ¡metro | DescripciÃ³n | Default |
|-----------|-------------|---------|
| `reranker_top_n` | Documentos finales tras reranking | 6 |
| `retrieval_k` | Documentos antes del reranking | 12 |
| `chunk_size` | TamaÃ±o de chunks de texto | 1500 |
| `chunk_overlap` | Solapamiento entre chunks | 300 |
| `bm25_weight` | Peso de bÃºsqueda lÃ©xica | 0.4 |
| `vector_weight` | Peso de bÃºsqueda vectorial | 0.6 |

### Modelos Disponibles

**LLM (Ollama):**
- `qwen2.5` - Recomendado para espaÃ±ol
- `qwen3` - Nueva versiÃ³n con mejor razonamiento
- `gemma3` - Modelo de Google
- `deepseek-r1` - Optimizado para razonamiento

**Embeddings:**
- `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` - Default multilingÃ¼e
- `embeddinggemma` (Ollama) - 300M params, 100+ idiomas

## ğŸ”§ SoluciÃ³n de Problemas

### Ollama no disponible
```bash
# Verificar que Ollama estÃ© corriendo
ollama serve

# Descargar modelo si es necesario
ollama pull qwen2.5
```

### Base de datos corrupta
```bash
# Eliminar y reconstruir
rm -rf db/
uv run cerebro.py
```

### Contexto fragmentado
El sistema detecta automÃ¡ticamente documentos fragmentados y lee el contenido completo usando `read_full_document()`.

### Enlaces vacÃ­os en metadata
Si la base de datos fue creada antes de la extracciÃ³n de enlaces:
```bash
rm -rf db/
uv run cerebro.py
```

## ğŸ“‚ Estructura del Proyecto

```
ObsidianRAG/
â”œâ”€â”€ cerebro.py              # FastAPI server
â”œâ”€â”€ app.py                  # Streamlit UI
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py         # ConfiguraciÃ³n Pydantic
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ qa_agent.py         # LangGraph agent
â”‚   â”œâ”€â”€ qa_service.py       # Retriever hÃ­brido
â”‚   â”œâ”€â”€ db_service.py       # ChromaDB + indexaciÃ³n
â”‚   â””â”€â”€ metadata_tracker.py # DetecciÃ³n de cambios
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py           # ConfiguraciÃ³n de logging
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ debug/              # Utilidades de debug
â”‚   â””â”€â”€ tests/              # Tests de integraciÃ³n
â””â”€â”€ db/                     # Base de datos ChromaDB
```

## ğŸ”® Roadmap

- [ ] Selector de modelos en UI (qwen3, gemma3, deepseek)
- [ ] IntegraciÃ³n de embeddinggemma desde Ollama
- [ ] Soporte para APIs externas (Google AI)
- [ ] Modo conversacional con memoria
- [ ] Dashboard de analytics

## ğŸ“„ Licencia

MIT License - ver [LICENSE](LICENSE)

## ğŸ™ CrÃ©ditos

- [LangGraph](https://github.com/langchain-ai/langgraph) - Framework de agentes
- [Ollama](https://ollama.ai/) - LLMs locales
- [ChromaDB](https://www.trychroma.com/) - Base de datos vectorial
- [Streamlit](https://streamlit.io/) - Framework de UI
